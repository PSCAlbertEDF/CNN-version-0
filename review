Un general review: https://arxiv.org/pdf/1412.4183.pdf

Three Classification models for OCR:

1，比较著名的是Ian goodfellow在13年提出的 (Which is our Google Article)
multi-digit number classification([1312.6082] Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks)
同样也是基于deep CNN. 该方法的不足在于要事先选定可预测的sequence的最大长度，较适用于门牌号码或者车牌号码(少量字符, 且每个字符之间可以看作是独立);


2，另一类比较常用的方法是RNN/LSTM/GRU + CTC, 方法最早由Alex Graves在06年提出应用于语音识别。
这个方法的好处在于可以产生任意长度的文字，并且模型的性质决定了它有能力学到文字于文字之间的联系(temporal relations/dependencies)。
不足之处在于sequential natural决定了它的计算效率没有CNN高，并且还有潜在的gradients exploding/vanishing的问题

http://karpathy.github.io/2015/05/21/rnn-effectiveness/ a thorough overview on Recurrent Neural Networks 

3,另一类不需要对文字预先分割的方法就是attention-mechanism，attention可以分为hard attention和soft attention.
Franchement a la mode!!!

Recurrent Models of Visual Attention, 2014:
https://arxiv.org/abs/1406.6247

An article of OCR using Attention-Mechanism:
https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lee_Recursive_Recurrent_Nets_CVPR_2016_paper.pdf

A general introduction: http://www.cnblogs.com/shixiangwan/p/7573589.html


其中hard attention能够直接给出hard location，
通常是bounding box的位置 （https://arxiv.org/pdf/1412.7755.pdf), 想法直观，
缺点是不能直接暴力bp。soft attention通常是rnn/lstm/gru encoder-decoder model (https://arxiv.org/abs/1603.03101), 
可以暴力bp。

还有一种比较特别的gradient-based       attention(http://www.ics.uci.edu/~yyang8/research/feedback/feedback-iccv2015.pdf) 也挺有意思。



For understanding LSTM Networks:
http://colah.github.io/posts/2015-08-Understanding-LSTMs/


label recognition: https://github.com/dengdan/seglink#test-your-own-images
